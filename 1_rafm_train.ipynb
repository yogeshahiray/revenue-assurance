{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "is_executing": true,
    "scrolled": true,
    "tags": []
   },
   "source": [
    "!pip install onnx onnxruntime tf2onnx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependencies for the model training code:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import pickle\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might show TensorFlow messages, such as a \"Could not find TensorRT\" warning. You can ignore these messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV data\n",
    "\n",
    "The CSV data that you use to train the model contains the following fields:\n",
    "*  **Call_Duration**\n",
    "*  **Data_Usage**\n",
    "*  **SMS_Count**\n",
    "*  **Roaming_Indicator**\n",
    "*  **MobileWallet_Use**\n",
    "*  **Cost**\n",
    "*  **Cellular_Location_Distance**\n",
    "*  **Personal_Pin_Used** \n",
    "*  **Avg_Call_Duration**\n",
    "*  **Avg_Data_Usage**\n",
    "*  **Avg_Cost**\n",
    "*  **fraud** - If the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set the input (X) and output (Y) data. \n",
    "# The only output data is whether it's fraudulent. All other fields are inputs to the model.\n",
    "\n",
    "feature_indexes = [\n",
    "    0,  # Call_Duration\n",
    "    1,  # Data_Usage\n",
    "    2,  # SMS_Count\n",
    "    3,  # Roaming_Indicator\n",
    "    4,  # MobileWallet_Use\n",
    "    6,  # Cost\n",
    "    7,  # Cellular_Location_Distance\n",
    "    8,  # Personal_Pin_Used \n",
    "    9,  # Avg_Call_Duration\n",
    "    10, # Avg_Data_Usage\n",
    "    11  # Avg_Cost\n",
    "]\n",
    "\n",
    "label_indexes = [\n",
    "    12  # fraud\n",
    "]\n",
    "\n",
    "df = pd.read_csv('data/telecom_revass_data.csv')\n",
    "X = df.iloc[:, feature_indexes].values\n",
    "y = df.iloc[:, label_indexes].values\n",
    "\n",
    "print(df.info)\n",
    "\n",
    "print(df.head)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Path(\"artifact\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"artifact/test_data.pkl\", \"wb\") as handle:\n",
    "    pickle.dump((X_test, y_test), handle)\n",
    "with open(\"artifact/scaler.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(scaler, handle)\n",
    "\n",
    "# Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.ravel())\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The model is a simple, fully-connected, deep neural network, containing three hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=len(feature_indexes)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Training a model is often the most time-consuming part of the machine learning process.  Large models can take multiple GPUs for days.  Expect the training on CPU for this very simple model to take a minute or more."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model and get performance\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 2\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    verbose=True,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training of model is complete. Took {end-start} seconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Normally we use tf2.onnx.convert.from_keras.\n",
    "# workaround for tf2onnx bug https://github.com/onnx/tensorflow-onnx/issues/2348\n",
    "\n",
    "# Wrap the model in a `tf.function`\n",
    "@tf.function(input_signature=[tf.TensorSpec([None, X_train.shape[1]], tf.float32, name='dense_input')])\n",
    "def model_fn(x):\n",
    "    return model(x)\n",
    "\n",
    "# Convert the Keras model to ONNX\n",
    "model_proto, _ = tf2onnx.convert.from_function(\n",
    "    model_fn,\n",
    "    input_signature=[tf.TensorSpec([None, X_train.shape[1]], tf.float32, name='dense_input')]\n",
    ")\n",
    "\n",
    "# Save the model as ONNX for easy use of ModelMesh\n",
    "os.makedirs(\"models/fraud/1\", exist_ok=True)\n",
    "onnx.save(model_proto, \"models/fraud/1/rafmmodel.onnx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might include TensorFlow messages related to GPUs. You can ignore these messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the model file was created successfully\n",
    "\n",
    "The output should include the model name, size, and date. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "! ls -alRh ./models/"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pickle\n",
    "import onnxruntime as rt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data and scaler:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "with open('artifact/scaler.pkl', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "with open('artifact/test_data.pkl', 'rb') as handle:\n",
    "    (X_test, y_test) = pickle.load(handle)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ONNX inference runtime session and predict values for all test inputs:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "sess = rt.InferenceSession(\"models/fraud/1/rafmmodel.onnx\", providers=rt.get_available_providers())\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "y_pred_temp = sess.run([output_name], {input_name: X_test.astype(np.float32)}) \n",
    "y_pred_temp = np.asarray(np.squeeze(y_pred_temp[0]))\n",
    "threshold = 0.95\n",
    "y_pred = np.where(y_pred_temp > threshold, 1, 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "y_test_arr = y_test.squeeze()\n",
    "correct = np.equal(y_pred, y_test_arr).sum().item()\n",
    "acc = (correct / len(y_pred)) * 100\n",
    "precision = precision_score(y_test_arr, np.round(y_pred))\n",
    "recall = recall_score(y_test_arr, np.round(y_pred))\n",
    "\n",
    "print(f\"Eval Metrics: \\n Accuracy: {acc:>0.1f}%, \"\n",
    "      f\"Precision: {precision:.4f}, Recall: {recall:.4f} \\n\")\n",
    "\n",
    "c_matrix = confusion_matrix(y_test_arr, y_pred)\n",
    "ConfusionMatrixDisplay(c_matrix).plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example: Is below transaction likely to be fraudulent?\n",
    "\n",
    "Here is the order of the fields from Sally's transaction details:\n",
    " Call_Duration 10\n",
    " \n",
    " Data_Usage 300\n",
    " \n",
    " SMS_Count 5\n",
    " \n",
    " Roaming_Indicator 0\n",
    " \n",
    " MobileWallet_Use 1\n",
    " \n",
    " Cost 50\n",
    " \n",
    " Cellular_Location_Distance 3\n",
    " \n",
    " Personal_Pin_Used  20\n",
    " \n",
    " Avg_Call_Duration 12\n",
    " \n",
    " Avg_Data_Usage 350\n",
    " \n",
    " Avg_Cost 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# No-fraud\n",
    "# Call_Duration 10\n",
    "# Data_Usage 300\n",
    "# SMS_Count 5\n",
    "# Roaming_Indicator 0\n",
    "# MobileWallet_Use 1\n",
    "# Cost 50\n",
    "# Cellular_Location_Distance 3\n",
    "# Personal_Pin_Used  20\n",
    "# Avg_Call_Duration 12\n",
    "# Avg_Data_Usage 350\n",
    "# Avg_Cost\n",
    "\n",
    "\n",
    "telco_transaction_details = [\n",
    "    [10,\n",
    "    300,\n",
    "    5,\n",
    "    0,\n",
    "    1,\n",
    "    50,\n",
    "    3,\n",
    "    20,\n",
    "    12,\n",
    "    350,\n",
    "     0\n",
    "     ]\n",
    "    ]\n",
    "prediction = sess.run([output_name], {input_name: scaler.transform(telco_transaction_details).astype(np.float32)})\n",
    "\n",
    "print(\"Is transaction predicted to be fraudulent? (true = YES, false = NO) \")\n",
    "print(np.squeeze(prediction) > threshold)\n",
    "\n",
    "print(\"How likely was this transaction to be fraudulent? \")\n",
    "print(\"{:.5f}\".format(100 * np.squeeze(prediction)) + \"%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Is below transaction likely to be fraudulent?\n",
    "\n",
    "Here is the order of the fields from Sally's transaction details:\n",
    " #Fraud#\n",
    " \n",
    " Call_Duration 300\n",
    " \n",
    " Data_Usage 10000\n",
    " \n",
    " SMS_Count 50\n",
    " \n",
    " Roaming_Indicator 1\n",
    " \n",
    " MobileWallet_Use 1\n",
    " \n",
    " Cost 500\n",
    " \n",
    " Cellular_Location_Distance 100\n",
    " \n",
    " Personal_Pin_Used  1\n",
    " \n",
    " Avg_Call_Duration 50\n",
    " \n",
    " Avg_Data_Usage 8000\n",
    " \n",
    " Avg_Cost 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "telco_transaction_details = [\n",
    "    [300,\n",
    "    10000,\n",
    "    50,\n",
    "    1,\n",
    "    1,\n",
    "    500,\n",
    "    100,\n",
    "    1,\n",
    "    50,\n",
    "    8000,\n",
    "     0\n",
    "          ]\n",
    "    ]\n",
    "\n",
    "prediction = sess.run([output_name], {input_name: scaler.transform(telco_transaction_details).astype(np.float32)})\n",
    "\n",
    "print(\"Is transaction predicted to be fraudulent? (true = YES, false = NO) \")\n",
    "print(np.squeeze(prediction) > threshold)\n",
    "\n",
    "print(\"How likely was this transaction to be fraudulent? \")\n",
    "print(\"{:.5f}\".format(100 * np.squeeze(prediction)) + \"%\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "63462a1f26ab486248b2a0fd058a0d9f9a6566a80083a3e1eb8f35617f2381b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
